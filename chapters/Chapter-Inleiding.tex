\begin{savequote}[0.55\linewidth]
	``The future belongs to those who believe in the beauty of their dreams.''
	\qauthor{\textasciitilde 
    Eleanor Roosevelt}
\end{savequote}

\chapter{Inleiding}
Het Web zorgt ervoor dat heel wat informatie beschikbaar is voor mensen en gedeeld kan worden door mensen. De webpagina’s zijn dan ook makkelijk toegankelijk voor iedereen die beschikt over een internetverbinding. Helaas betekent dit niet dat machines de gegevens even makkelijk kunnen decoderen. Om dagelijkse taken uit te voeren is menselijke interactie dan ook essentieel. Wanneer we zonder tussenkomst van machines een daguitstap plannen dan is dit op zich al een zeer complex proces. Zo moet onder andere de agenda van alle betrokken personen vergeleken worden om te weten te komen wanneer (bijna) iedereen beschikbaar is. Er moet gecontroleerd worden of de weersverwachtingen ideaal zijn voor de uitstap. Ook moet er rekening gehouden worden met de interesses van de verschillende personen om te beslissen welk type uitstap gedaan zal worden. Indien we dit proces willen laten overnemen door \textit{intelligent agents} dan houdt dit per definitie in dat we machines toegang moeten kunnen verlenen tot allerhande persoonlijke informatie.

Verder zijn gegevens ook niet voor iedereen beschikbaar. Grote spelers zoals Google, Instagram, Linkedin hebben rechtstreeks toegang tot persoonlijke gegevens van mensen. Ze kunnen deze gegevens zelf bijhouden, opvragen en controleren. Wanneer verschillende bedrijven dezelfde gegevens van mensen gaan opslaan, ontstaan er duplicate data. Dit maakt het proces opnieuw complexer. Bovendien moet in deze ook rekening gehouden worden met de wetten op de privacy, die overigens per land verschillend zijn.  Een mogelijke oplossing om deze problemen te omzeilen is de verkregen info decentraliseren. Dit wil concreet zeggen dat de gebruiker zelf controle heeft over zijn gegevens. Bedrijven die info willen over bepaalde personen, zullen dit zelf bij de gebruikers moeten opvragen. De gegevens worden dus niet bijgehouden in echte databanken, maar in andere bronnen waarop verder in deze masterproef op ingegaan zal worden.

Bovendien is het moeilijk om te werken met geografische data, omdat hier weinig implementaties van gemaakt zijn. Het geografische is ook eerder wiskundig om op te lossen. Deze masterproef legt de focus op het werken met geografische gegevens.

\section{Overzicht}

In \chapterref{chap:literatuurstudie} wordt de \textit{state of the art} behandeld, waarbij in details ingegaan wordt op technologieën/technieken al bestaan. Hierbij geeft \sectionref{sec:semantic_web} uitleg over het Semantisch Web. Vervolgens leggen \sectionref{sec:linked_data}, \sectionref{sec:rdf} en \sectionref{sec:sparql} de gebruikte technologieën uit, namelijk Linked Data, RDF en SPARQL. \sectionref{sec:comunica} vertelt meer over Comunica. Comunica brengt de (net hiervoor) genoemde technologieën bij elkaar in een implementatie. \sectionref{sec:ogc} vertelt over het OGC. Dit is een organisatie die standaarden voorziet om met geografische data te werken. Ten slotte beschrijft \sectionref{sec:geosparql} een specifieke standaard van het OGC, namelijk GeoSPARQL. Hierbij is \sectionref{sec:geosparql} zo belangrijk omdat het onderzoek (zie \sectionref{sec:onderzoeksvraag}) een implementatie van GeoSPARQL vereist. Zo zal \sectionref{sec:geosparql} uitleggen waar rekening mee moet gehouden worden om de implementatie te maken.

In \chapterref{chap:implementatie} wordt de eigen implementatie uitgelegd van GeoSPARQL. Zo legt \sectionref{sec:impl_comunica} uit waarom Comunica zo een belangrijke rol speelt bij deze implementatie. Dit wordt gevolgd door \sectionref{sec:datastructuur}, \sectionref{sec:topologische_functies}, \sectionref{sec:niet_topologische_functies} en \sectionref{sec:projecties} die beschrijven welke keuzes gemaakt zijn voor de verschillende aspecten van de implementatie. Daarnaast beschrijft \sectionref{sec:testomgeving} hoe de testomgeving gemaakt is. Ten slotte wordt het voorgaande nogmaals overlopen in \sectionref{sec:impl_overzicht} om te verduidelijken hoe alles exact samenwerkt. Hierbij wordt ook aangehaald welke verbeteringen nog dienen gemaakt te worden in toekomstig werk.

Vervolgens wordt in \chapterref{chap:interfaces} beschreven hoe het effectieve testen van de onderzoeksvraag gebeurt. Hiervoor wordt de hierboven beschreven testomgeving gebruikt. In \sectionref{sec:testset} wordt uitlegd welke use-case en dataset gebruikt zijn voor het testen van dit onderzoek. Vervolgens wordt in \sectionref{sec:data-dump}, \sectionref{sec:impl_tpf_interface} en \sectionref{sec:impl_sparql_endpoint} gecontroleerd of de hypothesen (zie \sectionref{sec:onderzoeksvraag}) voldaan zijn.

Ten slotte zal in \chapterref{chap:conclusie} een uiteindelijke conclusie getrokken worden. Hier wordt geantwoord op de vraag welke ``Linked data publicatie''-interfaces uitgebreid kunnen worden met GeoSPARQL-functionaliteiten door de filtering op de client uit te voeren.


\section{Probleemstelling en doel}
\label{sec:probleemstelling_doel}
Het creëren van een Semantisch Web staat nog in zijn kinderschoenen, met al enkele jaren onderzoek op de teller. Hoewel er al veel vooruitgang geboekt is, vereist het nog steeds zeer veel werk. Het ophalen van gegevens op het internet is reeds mogelijk door \textit{query engines} zoals onder andere Comunica en Virtuoso. Er is echter een veel beperkter aanbod aan mogelijkheden om met geografische informatie te werken. De bestaande implementaties van GeoSPARQL zijn incompleet of niet voldoende meegaand met de regels die opgesteld zijn door het OGC. 

Bovendien is het met huidige implementaties niet mogelijk om te queryen over verschillende bronnen of bronnen van verschillende types. Een simpel voorbeeld om dit probleem te verduidelijken, is het volgende. Stel: de Belgische overheid heeft een dataset die de volledige grens van België beschrijft (aan de hand van OGC standaarden). Daarnaast bevat deze dataset een soortgelijke beschrijving van de gewesten, provincies, gemeenten, steden en wegen. Op die manier zou het mogelijk zijn om op te vragen welke gemeenten of steden binnen een een bepaalde provincie liggen. Daarnaast zou het mogelijk zijn om te vragen welke steden of wegen op een bepaalde afstand (of interessanter: een kleinere afstand) van een stad liggen. Veronderstel nu dat de Franse, Nederlandse en Duitse overheden beschikken over een gelijkaardige dataset. Dan zouden soortgelijke opvragingen in deze dataset gedaan kunnen worden. Het zou echter onmogelijk zijn te weten welke Franse (of Nederlandse of Duitse) steden op een bepaalde afstand van een Belgische stad liggen. 

Dit probleem zou niet voorkomen wanneer de techniek van Comunica uitgebreidt kan worden naar de functionaliteiten van GeoSPARQL. Deze masterproef zal een simpele dataset voorzien die een soortgelijk probleem als hierboven kan simuleren. Hierbij zal gebruik gemaakt worden van Comunica, om zo gebruik te maken van de reeds voorziene mogelijkheden om te queryen over heterogene interfaces. 


\section{Onderzoeksvraag}
\label{sec:onderzoeksvraag}
Zoals beschreven in \sectionref{sec:probleemstelling_doel} zal onderzocht worden in welke mate Comunica uitgebreid kan worden om de GeoSPARQL functionaliteiten te ondersteunen. Aangezien meerdere bronnen van verschillende types gebruikt kunnen worden, moet de filtering zelf op de client gebeuren. Dit wordt geformuleerd in een onderzoeksvraag die uiteindelijk in \chapterref{chap:conclusie} beantwoord zal worden. 

\textbf{Onderzoeksvraag} Welke ``Linked Data publicatie''-interfaces kunnen uitgebreid worden met GeoSPARQL-functionaliteiten door de filtering op de client uit te voeren?

Wanneer Comunica zelfstandig een bestand moet ophalen en vervolgens queryen, is het mogelijk dat dit bestand geografische gegevens bevat. Deze queries moeten afgehandeld kunnen worden op zo een manier dat topologische (en niet-topologische) relaties berekend kunnen worden. Deze bron bevat zelf geen logica en wordt vervolgens de \textit{baseline}.

\textbf{Hypothese 1} Het is mogelijk om GeoSPARQL queries uit te voeren over ``data dumps'' waarbij de filtering op de client-side gebeurt.

Wanneer de bron een \acrfull{tpf} interface is, is er een server die de gegevens aanbiedt. Hierbij is het opnieuw mogelijk dat de dataset geografische informatie bevat. Door het filteren op de server moet het wederom mogelijk zijn om GeoSPARQL opvragingen uit te voeren.

\textbf{Hypothese 2} Het is mogelijk om GeoSPARQL queries uit te voeren over ``\acrshort{tpf} interfaces'' door de filtering op de client uit te voeren.

Een dataset kan ook vrijgegeven worden aan de hand van een SPARQL \textit{endpoint}. Comunica heeft de functionaliteit om te queryen naar een SPARQL \textit{endpoint}. Hierbij is het niet vanzelfsprekend dat GeoSPARQL queries opgevraagd kunnen worden aan een SPARQL \textit{endpoint}. Dit is echter wel mogelijk door de filtering op de client-side te doen.

\textbf{Hypothese 3} Het uitvoeren van GeoSPARQL queries op een ``SPARQL \textit{endpoint}'' is niet vanzelfsprekend. Het is echter mogelijk door de filtering op de client uit te voeren.